{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is my documentation file for all Pytorch commands and functions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # Generic Code Samples\n",
    "                                 #Imports\n",
    "# import torch\n",
    "# import torch.nn as nn                 # prebuilt neural net layers\n",
    "# import torch.nn.functional as F       # activations functions\n",
    "# import torch.optim as optim           # optimizer functions\n",
    "                                #Functions\n",
    "                                  #many functions can be written explicitly, such as torch.func_name(tens_name) or implicitly as in tens_name.func_name()\n",
    "                                    #Creating tensors\n",
    "# torch.tensor([#,#,#],[#,#,#])         # creates a tensor with rows and columns of the input #s\n",
    "# torch.tensor(list_name)               # creates tensor with data type inferred from input (Copy) (Factory)\n",
    "# torch.as_tensor(list_name)            # (Share) (Higher performance at scale because there's only one total instance of the tensor) (Factory)\n",
    "# torch.eye(#)                          # creates a tensor with the “identity matrix” with # of rows (1’s along the diagonal)\n",
    "# torch.diag(torch.ones(3))             # same as using the .eye(#) function\n",
    "# torch.zeros(#,#)                      # creates a tensor of shape #,# with all zeros\n",
    "# torch.ones(#,#)                       # creates tensor of shape #,# with all one's\n",
    "# torch.rand(#,#)                       # creates tensor of shape #,# with all random values between 0 and 1\n",
    "# torch.arange(start=#, end=#, step=#)  # creates a tensor starting with the first #, ending at the 2nd # (not inclusive), with a step size of the 3rd # - also, do not have to include these words\n",
    "# torch.linspace(start=#, end=#, steps=#)   # creates a tensor starting with the first #, ending at the 2nd # (is inclusive), with the 3rd # of steps inbetween - also do not have to include these words\n",
    "# torch.from_numpy(array_name)          # creates a tensor from a numpy array\n",
    "\n",
    "                                    #Manipulating tensors\n",
    "# sorted_tens_1, indices = torch.sort(tens_1, descending=False)\n",
    "# torch.where(tens_>#, tens_, tens_*2)  # like a small lambda function, finds where values > #, sets value = to middle value, where this is not true, does 3rd operation\n",
    "# tens_name.permute(#,#,#)              # switches the shape of the tensor around, using the orginal index values. IE, if you wanted a tensor of shape (50, 25, 30) to be (30, 25, 50) in this function you would input (2, 1, 0) # .t() is for ease of use, with only 2 dimensional tensors.\n",
    "# tens_name.squeeze(#)                  # removes a dimension from a tensor, number here is the index of the dimension you want to remove.\n",
    "# tens_name.unsqueeze(#)                # adds a dimension to a tensor, number here is the index where the new dimension should be placed, either infront or behind the indexes of the current dimensions.\n",
    "\n",
    "                                #Methods\n",
    "# tens_name.reshape(#,#)                # reshapes the tensor, (new shape must have same amount of elements) can put (1,-1) in here, as a keyword to flatten the tensor\n",
    "# tens_name.view(#,#)                   # reshapes the tensor, only works on tensors in the same contigious memory block, and therefore can be more efficient at times, I probably wont use this for now.\n",
    "# tens_name.short()                     # converts to int 16 (default is int 32)\n",
    "# tens_name.half()                      # converts to float 16\n",
    "# tens_name.float()                     # converts to float 32\n",
    "# tens_name.long()                      # converts to int 64\n",
    "# tens_name.double()                    # converts to float 64\n",
    "# tens_name.bool()                      # converts to boolean\n",
    "# tens_name.numpy()                     # converts to numpy array\n",
    "\n",
    "                                #Attributes\n",
    "# tens_name.shape()                     # returns shape of tensor (.size is the exact same thing)\n",
    "# tens_name.numel()                     # returns the number of elements in the tensor\n",
    "# tens_name.dtype                       # returns data type\n",
    "# tens_name.device                      # returns gpu/cpu\n",
    "# tens_name.layout                      #\n",
    "# tens_name.ndimension()                # returns # of dimensions of the tensor\n",
    "\n",
    "                                # Indexing tensors\n",
    "# tens_name[#]                          # returns entire row\n",
    "# tens_name[:,#]                        # returns entire column\n",
    "# tens_name[(tens_name > # | tens_name < #    # returns elements above OR below these #s, can also use the & sign here\n",
    "\n",
    "                                # Basic Tensor Math Operations\n",
    "                                    # I can add a _ after any of these functions to do these operations, \"in place,\" which takes less time and memory\n",
    "                                    # broadcasting is a built-in feature for ease of use which allows you to do operations between tensors of different sizes\n",
    "                                    # in each of these parenthesis, you can also specify dim=#, to just do the operation on a specific row. When specifying an axis, the index returned is relative to that axis.\n",
    "# tens_name.sum()                       # adds all the elements of a tensor into one scalar value.\n",
    "# tens_name.prod()                      # multiples all elements\n",
    "# tens_name.std()                       # std all elements\n",
    "# tens_name.max()                       # returns the max value of all the elements in the array.\n",
    "# tens_name.mean()                      # returns a single value tensor of the avg of all elements\n",
    "# tens_name.mean().item()               # returns a single scalar value\n",
    "# tens_name.argmax()                    # returns index of max value of the elements. index value returned is as if the function was flattened to a 1D array.\n",
    "# tens_name.argmin()                    # returns index of min value of the elements. index value returned is as if the function was flattened to a 1D array.\n",
    "# tens_name.unique()                    # returns the unique values of the tensor\n",
    "# tens_name_1 += tens_name_2            # Easiest way to write Add/Subtract/Multiply/Divide, in place\n",
    "# tens_name < #                         # returns a tensor containing all elements less than this #\n",
    "# tens_name ** #                        # raises all elements by this power\n",
    "# tens_name.mm(tens_name_2)             # raises all elements by the element wise numbers in another matrix\n",
    "# tens_name.cat(tens_name_2, dim=#)     # this adds tensors together similar to a \"union\" in SQL, the dim specifies whether you want to add the columns together or the rows, left blank it will do both.\n",
    "# tens_name.clamp(max=#)                # sets all elements above this #, to this #, can also use min=# here\n",
    "# tens_name.abs()                       # returns the absolute value of each element\n",
    "# tens_name.any()                       # returns true if any values are true\n",
    "# tens_name.all()                       # returns true if all values are true\n",
    "# tens_name_1.dot(tens_name_2)          # dot product (multiples matrices together and adds all elements to return one single value).\n",
    "# torch.eq(tens_name_1, tens_name_2)    # (explicit example) returns a boolean tensor where elements of two tensors are equal\n",
    "# longer setup                          # can also do \"batch multiplication\" which is like \"multiple\" multiplication in one, not sure where Id end up using this though\n",
    "\n",
    "\t                                #Functions for creating NNs\n",
    "# class cls_name(nn.Module):                 # Creates a class of functions\n",
    "# \t  super().func_name()                 # initiates the nn.module part of the class function\n",
    "#     self.lay_name_1 = nn.Linear(input #, output #)        # creates a fully connected layer - For 3 hidden layers, repeat this 4 times, because the first is the input layer, and the last line contains the hidden layer, and the output layer.\n",
    "#     self.lay_name_2 = nn.Linear(input #, output #)        # creates a fully connected layer\n",
    "#     self.lay_name_3 = nn.Linear(input #, output #)        # creates a fully connected layer\n",
    "#     self.lay_name_4 = nn.Linear(input #, output #)        # creates a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            #EXAMPLE CODE\n",
    "                                    #Example code to create simple FF NN with 3 hidden layers\n",
    "\n",
    "\n",
    "# def forward(self, var_name1):\n",
    "#     var_name = F.act_func(self.lay_name1(var_name1))\n",
    "#     var_name = F.act_func(self.lay_name2(var_name1))\n",
    "#     var_name = F.act_func(self.lay_name3(var_name1))\n",
    "#     var_name = self.lay_name4(var_name1)  ## this may be unnecessary\n",
    "#     return F.loss_func(var_name1, dim=1)\n",
    "#\n",
    "# opt_var_name = optim.Adam(net.parameters(), lr=0.001)\n",
    "#\n",
    "# EPOCHS = 3\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for data in trainset:\n",
    "#         X, y = data\n",
    "#         net.zero_grad()\n",
    "#         output = net(X.view(-1, input  # )\n",
    "#         loss_var_name = F.loss_func_name(output, y)\n",
    "#         loss_var_name.backward()\n",
    "#         opt_var_name.step()\n",
    "#\n",
    "                    # measuring accuracy\n",
    "# Correct = 0\n",
    "# Total = 0\n",
    "# with torch.no_grad()\n",
    "#    for data in trainset:\n",
    "#        X, y = data\n",
    "#        Output = net(X.view(-1, 784)\n",
    "#             for idx, i in enumerate(output):\n",
    "#                 if torch.argmax(i) == y[idx]:\n",
    "#                     correct += 1\n",
    "#                     total += 1\n",
    "#        print(“Accuracy “, round(correct / total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            # EXAMPLE CODE OF BASIC FUNCTIONS -- All confirmed working 11/11/20\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#\n",
    "# my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=device, requires_grad=True)\n",
    "# print(my_tensor)\n",
    "# print(my_tensor.dtype)\n",
    "# print(my_tensor.device)\n",
    "# print(my_tensor.shape)\n",
    "# print(\"requires grad? {}\".format(my_tensor.requires_grad))\n",
    "# print()\n",
    "\n",
    "# x = torch.empty(size=(3,3))\n",
    "# print(x)\n",
    "# x = torch.zeros(3,3)\n",
    "# print(x)\n",
    "# x = torch.ones(3,3)\n",
    "# print(x)\n",
    "# x = torch.rand(3,3)\n",
    "# print(x)\n",
    "# x = torch.eye(3,3)\n",
    "# print(x)\n",
    "# x = torch.arange(0, 5, 1)\n",
    "# print(x)\n",
    "# x = torch.linspace(.1,1,10)\n",
    "# print(x)\n",
    "# x = torch.empty(size=(1,5)).normal_(mean=0, std=1)\n",
    "# print (x)\n",
    "# x = torch.empty(size=(1,5)).uniform_(0,1)\n",
    "# print(x)\n",
    "# x = torch.diag(torch.ones(3))    # same as using the .eye function\n",
    "# print(x)\n",
    "\n",
    "# print(x.bool())\n",
    "# print(x.short())\n",
    "# print(x.long())\n",
    "\n",
    "# np_array = np.zeros((5, 5))\n",
    "# print(np_array.)\n",
    "# tensor = torch.from_numpy(np_array)\n",
    "# print(tensor.type)\n",
    "# np_back = tensor.numpy()\n",
    "# print(np_back.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                # EXAMPLE CODE OF 3 Layer NN for MNIST CV\n",
    "                                                    # Contains: Imports, loading data, create NN, hyperparams, initalize NN, Loss and Optimizer, training, accuracy check\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "start = timer()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                                                    # Deterministic behavior (only use when comparing results/debugging because it slows the training time of NN)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "                                                    #Loading in MNIST Train and Test datasets from torchvision.datasets\n",
    "                                                        # MNIST dataset shape = batch_size, colors=1, 28 pixels, 28 pixels\n",
    "                                                        # datasets for images can have 2 different import files, one for the png images, the other, a csv for the labels of the images.\n",
    "                                                        # Depending on the objects your looking to detect, it can increase accuracy to apply a bunch of random transforms to the image, such as 5% of grayscale, 5% of 45 degree rotation, resize the image up 10%, crop it down 10%, random saturation 5%, etc. In this way it learns to really understand the relationships that make up an object, in less than ideal cases. But keep in mind, if you wouldnt recognize it, with the transforms, than the net probably shouldnt be trained like this, for example a 100% vertical flip on a digit, actually changes what digit it is, so more is not better at this point.\n",
    "                                                        # For FCNN, load as above\n",
    "                                                        # For CNN, use input_channel=1\n",
    "                                                        # For RNN, I consider it 28 time stamps, by 28 features, usually you wouldnt use an RNN for images though.\n",
    "my_transforms = transforms.Compose([                                 # create list of data transforms\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(.2860), std=(.3530))])    # This step increases accuracy of the majority of datasets, even MNIST)\n",
    "normalized = True\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='fashion_dataset/', train=True, transform=my_transforms, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='fashion_dataset/', train=False, transform=my_transforms, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                                                    # Hyperparameters\n",
    "                                                        # General HyperPs\n",
    "num_epochs = 1                                              # value represents how many times the loop runs to train the model on the same dataset)\n",
    "num_classes = 10\n",
    "learning_rate = .001\n",
    "                                                        # FC HyperPs\n",
    "# input_size = 784                                          # value represents pixels*pixels\n",
    "                                                        # CNN HyperPs\n",
    "in_channels = 1                                             # value represents amount of colors in image\n",
    "                                                        # RNN HyperPs\n",
    "# input_size = 28\n",
    "# sequence_length = 28\n",
    "# num_layers = 2\n",
    "# hidden_size = 256\n",
    "\n",
    "\n",
    "# def get_mean_std(loader):\n",
    "#     channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "#     for data, _ in loader:\n",
    "#         channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "#         channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "#         num_batches += 1\n",
    "#     mean = channels_sum/num_batches\n",
    "#     std = (channels_squared_sum/num_batches - mean**2)**0.5\n",
    "#     return mean, std\n",
    "#\n",
    "# mean, std = get_mean_std(train_loader)\n",
    "# print(mean)\n",
    "# print(std)\n",
    "\n",
    "                                    #General notes about NN types:\n",
    "                                        # RNN is the umbrella term, more specific types are gru's, and LSTMs, this RNN code can be very simply converted to these types by just changing \"RNN\" to \"GRU\" in the layer type. Then you can change the layer name in the NN function in 2 places as well.\n",
    "                                        # No need to create this class if you're using a prebuilt model, simply set model = prebuilt_name(arg, arg, arg) down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    # Create simple fully connected NN\n",
    "# nn_name = 'FCNN'\n",
    "# class NN(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(NN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 50)\n",
    "#         self.fc2 = nn.Linear(50, num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    #  Create simple Convolutional NN\n",
    "\n",
    "nn_name = 'CNN'\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # these last 3 arguments are chosen based on a simple formula which feeds the output of this layer into the next layer with the same dimensions\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride = (2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.fc1 = nn.Linear(16*7*7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    #  Create simple Recurrent NN - currently setup for normal LSTM, can be setup for a bidirectional LSTM, and a GRU \n",
    "                                        # a Bi-directional LSTM, is nearly the same as a normal LSTM, simply replace variable names for ease of understanding and change 3 things, set bidirectional=True and multiply hidden_size*2 and num_layers*2\n",
    "# nn_name = 'RNN'\n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(RNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #bidirectional=True\n",
    "#         self.fc = nn.Linear(hidden_size*sequence_length, num_classes)                   # adding hidden_size*sequence_length in the first arg will capture all the data from the hidden states, using just hidden_size, will only capture the information from the last hidden state, which will speed up training time, and can increase/decrease accuracy depending on dataset, since the last state already has second hand information from the previous hidden states.\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)       # num_layers*2 is B-LSTM specific\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)       # LSTM specific, num_layers*2 is B-LSTM specific\n",
    "#\n",
    "#         # Forward prop\n",
    "#         out, _ = self.lstm(x, (h0, c0))                                 # for RNN and GRU, this line only holds (x, h0), for the LSTM, it holds (x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])                                    # captures information from just last hidden state    #acc 93.4, time 240 sec, B-LTSM with same setup had acc 94.0, time 550 sec\n",
    "#         out = out.reshape(out.shape[0], -1)                           # captures information from all hidden states         #acc 96.1, time 320 sec\n",
    "#         out = self.fc(out)                                            # ^ included in line above\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    #Checkpoint functions\n",
    "# def save_checkpoint(state, filename=\"my_checkpoint.pt\"):             # this will overwrite the file, so for multiple runs, youll want to change this function slightly, or use a new file.\n",
    "#     print(\"-> Saving Checkpoint\")\n",
    "#     torch.save(state, filename)\n",
    "#\n",
    "# def load_checkpoint(checkpoint):\n",
    "#     print(\"-> Loading Checkpoint\")\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#\n",
    "# load_model = False                          # for saving results to build on later, cant set to true, until I have loaded data in the file already\n",
    "#\n",
    "# if load_model:\n",
    "#     load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"))\n",
    "\n",
    "                                    #Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape tests of model output with mock data\n",
    "                                        #FC NN\n",
    "# model = NN(784,10)\n",
    "# x = torch.random(64, 784)                # first # is the # of rows, 2nd is the # of features\n",
    "# print(model(x).shape)                    # checks that the output is correct\n",
    "                                        #CNN\n",
    "# model = CNN()\n",
    "# x = torch.randn(64, 1, 28, 28)\n",
    "# print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    # create model, set Optimizer and Loss function\n",
    "                                        #torch.torchvision.prebuilt_model_name(arg, arg, arg) contains alot of prebuilt NN structures\n",
    "\n",
    "# model = NN(input_size=input_size, num_classes=num_classes).to(device)        # FC NN\n",
    "model = CNN().to(device)                                                     # CNN\n",
    "# model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)   # RNN\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "                                        # Same for all\n",
    "criterion = nn.CrossEntropyLoss()                       # This line already contains, softmax, and negative log likelyhood\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.1, patience=5, verbose=True)\n",
    "                                        # For testing your NN on a single batch, add this line, and remove the (in enumerate) loop in the training loop.\n",
    "# data, targets = next(iter(train_loader))                # This line is for testing your NN on a single batch, you also comment out the for loop in the training loop, and \"de-dent\" the following lines\n",
    "\n",
    "                                    # NN Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "                                        # part of the save and load function\n",
    "    # if epoch % 2:\n",
    "    #     checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    #     save_checkpoint(checkpoint)\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for batch_idx, (data, targets) in loop:                # this is the loop that iterates over all batches, and the in enumerate part gives the index for each epoch\n",
    "            # Put data in Cuda if possible\n",
    "        targets = targets.to(device=device)\n",
    "        data = data.to(device=device)               # FC NN and CNN specific\n",
    "        # data = data.to(device=device).squeeze(1)  # RNN specific\n",
    "        # data = data.reshape(data.shape[0],-1)       # FC NN specific      # reshapes the 28,28 into a 784, by \"squeezing\" to a flat tensor\n",
    "\n",
    "            # forward function\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        #losses.append(loss.item())                         # for use in calculating the loss per epoch\n",
    "\n",
    "            # backward function\n",
    "        optimizer.zero_grad()                                             # this line tells the network to sum the gradient of each batch, individually, then add it to the total, much higher accuracy this way\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)   # this line works well with RNNs\n",
    "\n",
    "            # feature adjustments step function\n",
    "        optimizer.step()\n",
    "\n",
    "            # progress bar updates\n",
    "        loop.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        loop.set_postfix(loss = loss.item(), acc='place_holder') # The accuracy here is set to random right now\n",
    "\n",
    "        #mean_loss = sum(losses)/len(losses) \n",
    "        #scheduler.step(mean_loss)\n",
    "        #print(f'Loss at epoch {epoch} is {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Accuracy\n",
    "with open(\"nn_tests_output.txt\", \"a\") as output_file:\n",
    "    print(f'{nn_name}, Batch Size: {batch_size}, Epochs: {num_epochs}, Normalized: {normalized}, Parameters: {total_params}, Extra config: ', file = output_file)\n",
    "    def check_accuracy(loader, model):\n",
    "        if loader.dataset.train:\n",
    "            print('Checking accuracy on train set')\n",
    "        else:\n",
    "            print('Checking accuracy on test set')\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=device)#.squeeze(1)               # .squeeze(1) is all RNNs specific\n",
    "                y = y.to(device=device)\n",
    "                # x = x.reshape(x.shape[0], -1)                  # FC NN specific\n",
    "\n",
    "                scores = model(x)\n",
    "                _, predictions = scores.max(1)\n",
    "                num_correct += (predictions == y).sum()\n",
    "                num_samples += predictions.size(0)\n",
    "            print(f'Got {num_correct} / {num_samples} with accuracy of {float(num_correct)/float(num_samples)*100:.2f}', file=output_file)\n",
    "        model.train(x)\n",
    "\n",
    "    check_accuracy(train_loader, model),\n",
    "    check_accuracy(test_loader, model),\n",
    "    end = timer()\n",
    "    print(f'Time taken: {end-start:.1f} Seconds', file=output_file)\n",
    "    print('-------------------------------------------------------------------', file=output_file)\n",
    "\n",
    "\n",
    "# all these things, batch size, number of epochs, number of layers, have an effect on accuracy,\n",
    "# acc 96.9 after 20 epochs of FCNN with batch of 512 in 115 sec\n",
    "# acc 97.3 after 40 epochs of FCNN with batch 512, in 240 sec (after loading from 20 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work from Udacity Computer Vision Expert\n",
    "# Basic CNN Architecture with one Conv layer and one pooling layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, weight):\n",
    "        super(Net, self).__init__()\n",
    "        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n",
    "        k_height, k_width = weight.shape[2:]\n",
    "        # assumes there are 4 grayscale filters\n",
    "        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n",
    "        self.conv.weight = torch.nn.Parameter(weight)\n",
    "        # define a pooling layer\n",
    "        self.pool = nn.MaxPool2d(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculates the output of a convolutional layer\n",
    "        # pre- and post-activation\n",
    "        conv_x = self.conv(x)\n",
    "        activated_x = F.relu(conv_x)\n",
    "\n",
    "        # applies pooling layer\n",
    "        pooled_x = self.pool(activated_x)\n",
    "\n",
    "        # returns all layers\n",
    "        return conv_x, activated_x, pooled_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
